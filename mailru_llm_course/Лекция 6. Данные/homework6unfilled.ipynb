{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1460420,"sourceType":"datasetVersion","datasetId":856362}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge\n!pip install datasketch[scipy]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from rouge import Rouge \n\nimport transformers\nimport torch\n\nimport seaborn as sns\n\nimport hashlib\nimport datasketch\nimport pandas as pd\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hash(s: str) -> int:\n    return int(hashlib.sha256(s.encode('utf-8')).hexdigest(), 16) % 10**8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hash('вот такие пироги')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Инструкции","metadata":{"execution":{"iopub.status.busy":"2024-03-25T18:37:59.468870Z","iopub.execute_input":"2024-03-25T18:37:59.469648Z","iopub.status.idle":"2024-03-25T18:37:59.474007Z","shell.execute_reply.started":"2024-03-25T18:37:59.469605Z","shell.execute_reply":"2024-03-25T18:37:59.473070Z"}}},{"cell_type":"markdown","source":"Пример форматирования строк:","metadata":{}},{"cell_type":"code","source":"example = \"Эту строку мы хотим {verb}\"\nformat_inputs = \"форматировать\"\nexample.format(verb = format_inputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Задание 1\n\nДавайте на практике поймем, что такое системный промпт (system prompt), что такое инструкция (instruction), что такое контекст инструкции (inputs).\n\nНиже на английском языке даны три текста: один из них является системный промптом, другой - инструкцией (которая параметризуется контекстом), третий - контекст. Заполните переменные inputs, system_prompt и instruction с помощью переменных var_1, var_2, var_3, после чего сформируйте полный промпт. В ответе на задание укажите число - хеш от полученной строки промпта (код для хеширования указан)","metadata":{}},{"cell_type":"code","source":"var_1 = \"Pretend you are an student in Moscow State University. Write three reasons why do you love {inputs}.\"\nvar_2 = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\nvar_3 = \"large language models\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Заполните системный шаблон\ninputs = ...\nsystem_prompt = ...\ninstruction = ...","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"{system_prompt}\n\n### Instruction:\n{instruction}\n\n### Response:\n\"\"\".format(\n    system_prompt = system_prompt,\n    instruction = instruction,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(prompt)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Ответ:', hash(prompt))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretrained vs Instructed ","metadata":{}},{"cell_type":"markdown","source":"Воспользуемся полученной инструкцией и попробуем получить полезный ответ на нее с помощью предобученной модели. Сравним, насколько хорошо с этим справляется только предобученная (без дообучения) модель gpt2, которая училась правдоподобно продолжать текст, и насколько хорошо это делает та же gpt2, дообученная на инструкции.\n\nДалее возьмем эталонный вариант ответа на данную инструкцию и сравним, насколько генерации на нее похожи","metadata":{}},{"cell_type":"code","source":"device = 'cuda'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model_name = 'gpt2'\n\npretrained_model = transformers.AutoModelForCausalLM.from_pretrained(\n    pretrained_model_name,\n    torch_dtype=torch.float16,\n)\n\npretrained_model.to(device)\n\npretrained_tokenizer = transformers.AutoTokenizer.from_pretrained(pretrained_model_name)\npretrained_tokenizer.add_special_tokens({'pad_token': \"<|endoftext|>\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(42)\n\ninputs = pretrained_tokenizer(prompt, return_tensors=\"pt\").to(device)\noutput = pretrained_model.generate(**inputs, max_new_tokens=64)\n\npretrained_output = pretrained_tokenizer.decode(output.detach()[0])\nprint( pretrained_output )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda'\n\ninstructed_model_name = 'vicgalle/gpt2-open-instruct-v1'\n\ninstructed_model = transformers.AutoModelForCausalLM.from_pretrained(\n    instructed_model_name,\n    torch_dtype=torch.float16,\n)\n\ninstructed_model.to(device)\n\ninstructed_tokenizer = transformers.AutoTokenizer.from_pretrained(instructed_model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(42)\n\ninputs = instructed_tokenizer(prompt, return_tensors=\"pt\").to(device)\noutput = instructed_model.generate(**inputs, max_new_tokens=64)\ninstructed_output = instructed_tokenizer.decode(output.detach()[0])\n\nprint( instructed_output )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"golden_answer = \"\"\"As a student, I love large language models for several reasons:\n1. Efficiency: Large language models are incredibly efficient at processing and generating text.\n2. Versatility: Large language models can be used for a wide range of applications.\n3. Accuracy: Large language models are highly accurate and can produce high-quality results.\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RougeMetric:\n    def __init__(self):\n        self.rouge = Rouge()\n        \n    def __call__(self, hypothesis: str, reference: str) -> float:\n        scores = self.rouge.get_scores(hypothesis, reference)\n        return scores[0]['rouge-l']['f'] \n    \nrouge_wrapper = RougeMetric()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Задание 2\n\nНиже посчитаны две похожести двух текстов по метрике ROUGE:\n1 - похожесть сгенерированного предобученной моделью ответа на эталонный\n2 - похожесть сгенерированного инструктивной моделью ответа на эталонный\n\nПосчитайте, насколько качество инструктивной моделью по этой метрике лучше, чем качество только предобученной модели: посчитайте разность полученных метрик rouge (инструктивная минус предобученная). В качестве ответа укажите вещественное число, округленное до трех знаков","metadata":{}},{"cell_type":"code","source":"print('Rouge для генерации предобученной моделью:', \n      round(rouge_wrapper(pretrained_output, golden_answer), 3) )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Rouge для генерации инструктивной моделью:', \n      round(rouge_wrapper(instructed_output, golden_answer), 3) )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Ответ: ', round(..., 3) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Self instruct","metadata":{}},{"cell_type":"markdown","source":"На лекции разбирали подход self-instruct, который позволяет генерировать текстовые данные, обладающие нужными свойствами, с помощью большой языковой модели. Давайте посмотрим на один из шагов этого подхода: шаг, на котором мы подаем в llm запрос сгенерировать новый текст, похожий на предоставленные примеры","metadata":{}},{"cell_type":"markdown","source":"### Задание 3\n\nМы привели 3 примера для llm, на основании которых надо придумать четвертый и, возможно, следующие. Запустите код и посмотрите, какой новый пример сгенерировала llm. В качестве ответа укажите число - хеш от всей новой строки (с помощью кода ниже)","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nBelow are several tasks. Continue this list of tasks:\n1. Find the name for new startup with using large language models.\n2. Write three arguments why do you like pancakes.\n3. Describe the content in the best news article in the world.\n4.\"\"\"\n\nseed_everything(42)\n\ninputs = instructed_tokenizer(prompt, return_tensors=\"pt\").to(device)\noutput = instructed_model.generate(**inputs, max_new_tokens=64)\ninstructed_output = instructed_tokenizer.decode(output.detach()[0])\n\nprint( instructed_output )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(instructed_output.split('\\n')[7])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Ответ: ', hash(instructed_output.split('\\n')[7]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bias Selection","metadata":{}},{"cell_type":"markdown","source":"Давайте на живом примере посмотрим, как выглядит bias selection, рассмотренный на лекции. \nЭта концепция заключается в том, что при обычном равномерном семплировании мы с большей вероятностью выберем элемент, наиболее часто представленный в выборке. \n\nВ случае, когда мы хотим сократить размер датасета, сохранив его репрезентативность, нас интересуют как самые частые значения, так и относительно редкие, тк они, как правило, несут важную информацию о, например, разделяющей границе для классов в областях с низкой плотностью наблюдений\n\nНиже создается выборка, в которое заведомо число 2 встречается чаще всего, а остальные значения представлены реже. Посмотрите, что произойдет, если мы сократим выборку в 10 раз с применением обычного семплирования вместо умной дедубликации.","metadata":{}},{"cell_type":"code","source":"seed_everything(42)\n\ndata = np.random.choice(np.arange(5), size=100, replace=True, \n                         p = [0.1, 0.1, 0.6, 0.1, 0.1], )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(data, return_counts=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(data, discrete=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(42)\n\nsampled = np.random.choice(data, size=10, replace=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print( np.unique(sampled, return_counts=True) )\n\nsns.histplot(sampled, discrete=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Задание 4\n\nОпределите, какие значения пропали из датасета после уменьшения выборки путем равномерного семплирования\n\nОтвет дайте в виде последовательности по убыванию, записанную подряд без пробелов. Если потеряны точки 0, 1, 2, 3 и 4 то ответ будет 43210","metadata":{}},{"cell_type":"markdown","source":"## MinHashLSH","metadata":{}},{"cell_type":"markdown","source":"Посмотрим на нечеткую дедубликацию с помощью MinHashLSH, и попробуем с помощью нее найти похожие новости (вернее, их заголовки). Это поможет нам понять, насколько датасет разнообразный, и, может быть, насколько он требует дедубликации\n\nИспользуемый датасет (в kaggle можно добавить в input):\nhttps://www.kaggle.com/datasets/vfomenko/russian-news-2020","metadata":{}},{"cell_type":"code","source":"input_path = '/kaggle/input/russian-news-2020/news.csv'\ndf = pd.read_csv(input_path)\ndf = df.dropna(subset=['title']).reset_index(drop=True)\nprint(df.shape)\ndf.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sents = df['title'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(32)\n\nset_dict={} \nnorm_dict={} \ncount=1\nfor question in tqdm([x for x in sents]):\n    temp_list = []\n    for shingle in question.split(' '):\n        temp_list.append(shingle.lower())\n    set_dict[\"m{0}\".format(count)] = set(temp_list)\n    norm_dict[\"m{0}\".format(count)] = question\n    count +=1\n    \n    \nnum_perm = 128\nmin_dict = {}\ncount2 = 1\nfor val in tqdm(set_dict.values()):\n    m = datasketch.MinHash(num_perm=num_perm)\n    for shingle in val:\n        m.update(shingle.encode('utf8'))\n    min_dict[\"m{}\".format(count2)] = m\n    count2 += 1\n    \n    \nlsh = datasketch.MinHashLSH(threshold=0.8, num_perm=num_perm)\nfor key in tqdm(min_dict.keys()):\n    lsh.insert(key,min_dict[key]) # insert minhash data structure","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"big_list = []\nfor query in min_dict.keys():\n    big_list.append(lsh.query(min_dict[query]))\n    \nlen(big_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"big_list = sorted(big_list, key = lambda w: len(w), reverse=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_show = 3\nshowed = 0\nmax_count_per_cluster = 5\n\nfor i, elems in enumerate(big_list):\n    if len(elems) > 1:\n        cluster_texts = [norm_dict[key] for key in elems[:max_count_per_cluster]]\n        print(i)\n        print('\\n'.join(cluster_texts))\n        print('\\n', '=' * 30, '\\n')\n        showed += 1\n        if showed >= to_show:\n            break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Задание 5\n\nВыведите количество lsh-кластеров, размер которых > 1. В качестве ответа используйте целое число ","metadata":{"execution":{"iopub.status.busy":"2024-03-28T19:57:35.024892Z","iopub.execute_input":"2024-03-28T19:57:35.025286Z","iopub.status.idle":"2024-03-28T19:57:35.032011Z","shell.execute_reply.started":"2024-03-28T19:57:35.025256Z","shell.execute_reply":"2024-03-28T19:57:35.030783Z"}}}]}